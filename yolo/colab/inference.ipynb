{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40f9VBOnRWJj"
      },
      "source": [
        "# YOLO in TensorFlow: Inference Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LarZmCip39it"
      },
      "source": [
        "This is a Google Colaboratory notebook file to demonstrate inference using the TensorFlow Model Garden implementation of YOLOv3 on a video stream from your webcam.\n",
        "\n",
        "First, clone the GitHub repo and import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rECnP_DMQnZY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224d2604-ec31-40f3-8028-29234c4895cc"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import pathlib\n",
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"TensorFlowModelGardeners\" in pathlib.Path.cwd().parts:\n",
        "  while \"TensorFlowModelGardeners\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('TensorFlowModelGardeners').exists():\n",
        "  !git clone --depth 1 https://github.com/PurdueCAM2Project/TensorFlowModelGardeners\n",
        "os.chdir('TensorFlowModelGardeners')\n",
        "!git pull\n",
        "!pip install -r yolo/requirements.txt\n",
        "\n",
        "from google.colab import output\n",
        "from IPython.display import JSON, HTML\n",
        "\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from official.core import train_utils\n",
        "from yolo import run as yolo_run\n",
        "from yolo.utils.demos import utils as demo_utils"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n",
            "Requirement already satisfied: absl-py==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 2)) (0.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 3)) (1.6.3)\n",
            "Requirement already satisfied: attrs==20.2.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 4)) (20.2.0)\n",
            "Requirement already satisfied: cachetools==4.1.1 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 5)) (4.1.1)\n",
            "Requirement already satisfied: certifi==2020.6.20 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 6)) (2020.6.20)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: dill==0.3.2 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 8)) (0.3.2)\n",
            "Requirement already satisfied: future==0.18.2 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 9)) (0.18.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 10)) (0.3.3)\n",
            "Requirement already satisfied: google-auth-oauthlib==0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 11)) (0.4.1)\n",
            "Requirement already satisfied: google-auth==1.21.1 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 12)) (1.21.1)\n",
            "Requirement already satisfied: google-pasta==0.2.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos==1.52.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 14)) (1.52.0)\n",
            "Requirement already satisfied: grpcio==1.32.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 15)) (1.32.0)\n",
            "Requirement already satisfied: h5py==2.10.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 16)) (2.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 17)) (2.10)\n",
            "Requirement already satisfied: importlib-metadata==1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 18)) (1.7.0)\n",
            "Requirement already satisfied: importlib-resources==3.0.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 19)) (3.0.0)\n",
            "Requirement already satisfied: keras-preprocessing==1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 20)) (1.1.2)\n",
            "Requirement already satisfied: markdown==3.2.2 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 21)) (3.2.2)\n",
            "Requirement already satisfied: more-itertools==8.5.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 22)) (8.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 23)) (1.19.5)\n",
            "Requirement already satisfied: oauthlib==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 24)) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum==3.3.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 25)) (3.3.0)\n",
            "Requirement already satisfied: promise==2.3 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 26)) (2.3)\n",
            "Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 27)) (3.13.0)\n",
            "Requirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 28)) (0.2.8)\n",
            "Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 29)) (0.4.8)\n",
            "Requirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 30)) (1.3.0)\n",
            "Requirement already satisfied: requests==2.24.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 31)) (2.24.0)\n",
            "Requirement already satisfied: rsa==4.6 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 32)) (4.6)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 33)) (1.4.1)\n",
            "Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 34)) (1.15.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit==1.7.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 35)) (1.7.0)\n",
            "Requirement already satisfied: tensorboard>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 36)) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-addons>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 37)) (0.12.1)\n",
            "Requirement already satisfied: tensorflow-datasets>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 38)) (4.0.1)\n",
            "Requirement already satisfied: tensorflow-estimator>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 39)) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-metadata==0.24.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 40)) (0.24.0)\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 41)) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 42)) (0.11.0)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 43)) (0.5.0)\n",
            "Requirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 44)) (1.1.0)\n",
            "Requirement already satisfied: tqdm==4.49.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 45)) (4.49.0)\n",
            "Requirement already satisfied: typeguard==2.9.1 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 46)) (2.9.1)\n",
            "Requirement already satisfied: urllib3==1.25.10 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 47)) (1.25.10)\n",
            "Requirement already satisfied: werkzeug==1.0.1 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 48)) (1.0.1)\n",
            "Requirement already satisfied: wheel==0.35.1 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 49)) (0.35.1)\n",
            "Requirement already satisfied: wrapt==1.12.1 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 50)) (1.12.1)\n",
            "Requirement already satisfied: zipp==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 51)) (3.1.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 52)) (1.7.12)\n",
            "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 53)) (1.21.0)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 54)) (1.5.10)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 55)) (4.1.3)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 56)) (1.1.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 58)) (7.0.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 59)) (0.8)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 60)) (0.4.0)\n",
            "Requirement already satisfied: tf_slim>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 61)) (1.1.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 62)) (0.29.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 63)) (3.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 64)) (5.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 66)) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 67)) (7.0.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 68)) (2.0.2)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 70)) (1.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 71)) (0.1.95)\n",
            "Requirement already satisfied: tflite_support in /usr/local/lib/python3.6/dist-packages (from -r yolo/requirements.txt (line 72)) (0.1.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth==1.21.1->-r yolo/requirements.txt (line 12)) (53.0.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets>=3.2.1->-r yolo/requirements.txt (line 38)) (0.1.5)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r yolo/requirements.txt (line 41)) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r yolo/requirements.txt (line 41)) (1.12)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->-r yolo/requirements.txt (line 52)) (0.17.4)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->-r yolo/requirements.txt (line 52)) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.6.7->-r yolo/requirements.txt (line 52)) (3.0.1)\n",
            "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->-r yolo/requirements.txt (line 53)) (1.0.3)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=0.31.0->-r yolo/requirements.txt (line 53)) (0.4.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->-r yolo/requirements.txt (line 54)) (2.8.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle>=1.3.9->-r yolo/requirements.txt (line 54)) (4.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->-r yolo/requirements.txt (line 56)) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r yolo/requirements.txt (line 63)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r yolo/requirements.txt (line 63)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r yolo/requirements.txt (line 63)) (2.4.7)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r yolo/requirements.txt (line 70)) (0.22.2.post1)\n",
            "Requirement already satisfied: pybind11>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from tflite_support->-r yolo/requirements.txt (line 72)) (2.6.2)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-bigquery>=0.31.0->-r yolo/requirements.txt (line 53)) (1.16.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle>=1.3.9->-r yolo/requirements.txt (line 54)) (1.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->seqeval->-r yolo/requirements.txt (line 70)) (1.0.0)\n",
            "\n",
            "!--PREPPING GPU--! \n",
            "1 Physical GPUs, 1 Logical GPUs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rwMRMTs5Fiq"
      },
      "source": [
        "After cloning the repo, build the model and load the Darknet (paper implementation) pretrained weights. This may take time since Colab needs to download the weights.\n",
        "\n",
        "A prediction function for the model is also created. The [`predict`](https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=en#predict) function is faster for batched inputs (many images being processed at the same time) than it is for single images, like used in this tutorial. It is used for the sake of example here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLqIfPcI5HEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c54fda4-f736-4d84-d0e3-9c025d182d62"
      },
      "source": [
        "# Try out yolov3.yaml and yolov4-tiny-eval.yaml as well for more fun\n",
        "task, model = yolo_run.load_model('yolo_custom', ['yolo/configs/experiments/yolov4-eval.yaml'])\n",
        "model.make_predict_function()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
            "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla T4, compute capability 7.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
            "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: Tesla T4, compute capability 7.5\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/mixed_precision/loss_scale.py:56: DynamicLossScale.__init__ (from tensorflow.python.training.experimental.loss_scale) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.keras.mixed_precision.LossScaleOptimizer instead. LossScaleOptimizer now has all the functionality of DynamicLossScale\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'num_classes': 80, '_input_size': None, 'min_level': 3, 'max_level': 5, 'boxes_per_scale': 3, 'base': {'type': None}, 'dilate': False, 'filter': {'iou_thresh': 0.2, 'nms_thresh': 0.9, 'ignore_thresh': 0.7, 'loss_type': 'ciou', 'max_boxes': 200, 'anchor_generation_scale': 416, 'use_nms': False, 'iou_normalizer': 0.07, 'cls_normalizer': 1.0, 'obj_normalizer': 1.0}, 'norm_activation': {'activation': 'mish', 'use_sync_bn': False, 'norm_momentum': 0.99, 'norm_epsilon': 0.001}, 'decoder_activation': 'leaky', '_boxes': ['[12.0, 16.0]', '[19.0, 36.0]', '[40.0, 28.0]', '[36.0, 75.0]', '[76.0, 55.0]', '[72.0, 146.0]', '[142.0, 110.0]', '[192.0, 243.0]', '[459.0, 401.0]']}\n",
            "InputSpec(shape=(None, None, None, 3), ndim=4)\n",
            "<tensorflow.python.keras.regularizers.L2 object at 0x7f7e7bb37470>\n",
            "DarkNet(model_id='cspdarknet53')\n",
            "Model: \"cspdarknet53\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "ConvBN_0_0 (ConvBN)             (None, None, None, 3 992         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_1_csp_down (CSPRoute)   ((None, None, None,  27392       ConvBN_0_0[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_1_0 (DarkResidual)      (None, None, None, 6 20864       DarkRes_1_csp_down[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_1_csp_connect (CSPConne (None, None, None, 6 12800       DarkRes_1_0[0][0]                \n",
            "                                                                 DarkRes_1_csp_down[0][1]         \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_2_csp_down (CSPRoute)   ((None, None, None,  91136       DarkRes_1_csp_connect[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_2_0 (DarkResidual)      (None, None, None, 6 41472       DarkRes_2_csp_down[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_2_1 (DarkResidual)      (None, None, None, 6 41472       DarkRes_2_0[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_2_csp_connect (CSPConne (None, None, None, 1 21248       DarkRes_2_1[0][0]                \n",
            "                                                                 DarkRes_2_csp_down[0][1]         \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_3_csp_down (CSPRoute)   ((None, None, None,  362496      DarkRes_2_csp_connect[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_3_0 (DarkResidual)      (None, None, None, 1 164864      DarkRes_3_csp_down[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_3_1 (DarkResidual)      (None, None, None, 1 164864      DarkRes_3_0[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_3_2 (DarkResidual)      (None, None, None, 1 164864      DarkRes_3_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_3_3 (DarkResidual)      (None, None, None, 1 164864      DarkRes_3_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_3_4 (DarkResidual)      (None, None, None, 1 164864      DarkRes_3_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_3_5 (DarkResidual)      (None, None, None, 1 164864      DarkRes_3_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_3_6 (DarkResidual)      (None, None, None, 1 164864      DarkRes_3_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_3_7 (DarkResidual)      (None, None, None, 1 164864      DarkRes_3_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_3_csp_connect (CSPConne (None, None, None, 2 83456       DarkRes_3_7[0][0]                \n",
            "                                                                 DarkRes_3_csp_down[0][1]         \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_4_csp_down (CSPRoute)   ((None, None, None,  1445888     DarkRes_3_csp_connect[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_4_0 (DarkResidual)      (None, None, None, 2 657408      DarkRes_4_csp_down[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_4_1 (DarkResidual)      (None, None, None, 2 657408      DarkRes_4_0[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_4_2 (DarkResidual)      (None, None, None, 2 657408      DarkRes_4_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_4_3 (DarkResidual)      (None, None, None, 2 657408      DarkRes_4_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_4_4 (DarkResidual)      (None, None, None, 2 657408      DarkRes_4_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_4_5 (DarkResidual)      (None, None, None, 2 657408      DarkRes_4_4[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_4_6 (DarkResidual)      (None, None, None, 2 657408      DarkRes_4_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_4_7 (DarkResidual)      (None, None, None, 2 657408      DarkRes_4_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_4_csp_connect (CSPConne (None, None, None, 5 330752      DarkRes_4_7[0][0]                \n",
            "                                                                 DarkRes_4_csp_down[0][1]         \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_5_csp_down (CSPRoute)   ((None, None, None,  5775360     DarkRes_4_csp_connect[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_5_0 (DarkResidual)      (None, None, None, 5 2625536     DarkRes_5_csp_down[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_5_1 (DarkResidual)      (None, None, None, 5 2625536     DarkRes_5_0[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_5_2 (DarkResidual)      (None, None, None, 5 2625536     DarkRes_5_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_5_3 (DarkResidual)      (None, None, None, 5 2625536     DarkRes_5_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "DarkRes_5_csp_connect (CSPConne (None, None, None, 1 1316864     DarkRes_5_3[0][0]                \n",
            "                                                                 DarkRes_5_csp_down[0][1]         \n",
            "==================================================================================================\n",
            "Total params: 26,652,512\n",
            "Trainable params: 26,617,184\n",
            "Non-trainable params: 35,328\n",
            "__________________________________________________________________________________________________\n",
            "64 seen\n",
            "major: 0\n",
            "minor: 2\n",
            "revision: 5\n",
            "iseen: 32032000\n",
            "{'_type': 'net', 'batch': 64, 'subdivisions': 8, 'width': 608, 'height': 608, 'channels': 3, 'momentum': 0.949, 'decay': 0.0005, 'angle': 0, 'saturation': 1.5, 'exposure': 1.5, 'hue': 0.1, 'learning_rate': 0.0013, 'burn_in': 1000, 'max_batches': 500500, 'policy': 'steps', 'steps': (400000, 450000), 'scales': (0.1, 0.1), 'mosaic': 1}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 32, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 3, 'stride': 2, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'route', 'layers': -2}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 32, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'route', 'layers': (-1, -7)}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 3, 'stride': 2, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'route', 'layers': -2}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 64, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'route', 'layers': (-1, -10)}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 3, 'stride': 2, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'route', 'layers': -2}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'route', 'layers': (-1, -28)}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 3, 'stride': 2, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'route', 'layers': -2}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'route', 'layers': (-1, -28)}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 1024, 'size': 3, 'stride': 2, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'route', 'layers': -2}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 3, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'shortcut', 'from': -3, 'activation': 'linear'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'route', 'layers': (-1, -16)}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 1024, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'mish'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 1024, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'maxpool', 'stride': 1, 'size': 5}\n",
            "{'_type': 'route', 'layers': -2}\n",
            "{'_type': 'maxpool', 'stride': 1, 'size': 9}\n",
            "{'_type': 'route', 'layers': -4}\n",
            "{'_type': 'maxpool', 'stride': 1, 'size': 13}\n",
            "{'_type': 'route', 'layers': (-1, -3, -5, -6)}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 1024, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'upsample', 'stride': 2}\n",
            "{'_type': 'route', 'layers': 85}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'route', 'layers': (-1, -3)}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 512, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 512, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'upsample', 'stride': 2}\n",
            "{'_type': 'route', 'layers': 54}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'route', 'layers': (-1, -3)}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 256, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 256, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 128, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 256, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'size': 1, 'stride': 1, 'pad': 1, 'filters': 255, 'activation': 'linear'}\n",
            "{'_type': 'yolo', 'mask': (0, 1, 2), 'anchors': [(12, 16), (19, 36), (40, 28), (36, 75), (76, 55), (72, 146), (142, 110), (192, 243), (459, 401)], 'classes': 80, 'num': 9, 'jitter': 0.3, 'ignore_thresh': 0.7, 'truth_thresh': 1, 'scale_x_y': 1.2, 'iou_thresh': 0.213, 'cls_normalizer': 1.0, 'iou_normalizer': 0.07, 'iou_loss': 'ciou', 'nms_kind': 'greedynms', 'beta_nms': 0.6, 'max_delta': 5}\n",
            "{'_type': 'route', 'layers': -4}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 2, 'pad': 1, 'filters': 256, 'activation': 'leaky'}\n",
            "{'_type': 'route', 'layers': (-1, -16)}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 512, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 512, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 256, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 512, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'size': 1, 'stride': 1, 'pad': 1, 'filters': 255, 'activation': 'linear'}\n",
            "{'_type': 'yolo', 'mask': (3, 4, 5), 'anchors': [(12, 16), (19, 36), (40, 28), (36, 75), (76, 55), (72, 146), (142, 110), (192, 243), (459, 401)], 'classes': 80, 'num': 9, 'jitter': 0.3, 'ignore_thresh': 0.7, 'truth_thresh': 1, 'scale_x_y': 1.1, 'iou_thresh': 0.213, 'cls_normalizer': 1.0, 'iou_normalizer': 0.07, 'iou_loss': 'ciou', 'nms_kind': 'greedynms', 'beta_nms': 0.6, 'max_delta': 5}\n",
            "{'_type': 'route', 'layers': -4}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 2, 'pad': 1, 'filters': 512, 'activation': 'leaky'}\n",
            "{'_type': 'route', 'layers': (-1, -37)}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 1024, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 1024, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'filters': 512, 'size': 1, 'stride': 1, 'pad': 1, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'batch_normalize': 1, 'size': 3, 'stride': 1, 'pad': 1, 'filters': 1024, 'activation': 'leaky'}\n",
            "{'_type': 'convolutional', 'size': 1, 'stride': 1, 'pad': 1, 'filters': 255, 'activation': 'linear'}\n",
            "{'_type': 'yolo', 'mask': (6, 7, 8), 'anchors': [(12, 16), (19, 36), (40, 28), (36, 75), (76, 55), (72, 146), (142, 110), (192, 243), (459, 401)], 'classes': 80, 'num': 9, 'jitter': 0.3, 'ignore_thresh': 0.7, 'truth_thresh': 1, 'random': 1, 'scale_x_y': 1.05, 'iou_thresh': 0.213, 'cls_normalizer': 1.0, 'iou_normalizer': 0.07, 'iou_loss': 'ciou', 'nms_kind': 'greedynms', 'beta_nms': 0.6, 'max_delta': 5}\n",
            "full net: \n",
            "608 608 3\tconvCFG(_type='convolutional', w=608, h=608, c=3, size=3, stride=1, pad=1, filters=32)\n",
            "608 608 32\tconvCFG(_type='convolutional', w=608, h=608, c=32, size=3, stride=2, pad=1, filters=64)\n",
            "304 304 64\tconvCFG(_type='convolutional', w=304, h=304, c=64, size=1, stride=1, pad=0, filters=64)\n",
            "304 304 64\trouteCFG(_type='route', w=304, h=304, c=64, layers=(-2,))\n",
            "304 304 64\tconvCFG(_type='convolutional', w=304, h=304, c=64, size=1, stride=1, pad=0, filters=64)\n",
            "304 304 64\tconvCFG(_type='convolutional', w=304, h=304, c=64, size=1, stride=1, pad=0, filters=32)\n",
            "304 304 32\tconvCFG(_type='convolutional', w=304, h=304, c=32, size=3, stride=1, pad=1, filters=64)\n",
            "304 304 64\tshortcutCFG(_type='shortcut', w=304, h=304, c=64, _from=(-3,), activation='linear')\n",
            "304 304 64\tconvCFG(_type='convolutional', w=304, h=304, c=64, size=1, stride=1, pad=0, filters=64)\n",
            "304 304 128\trouteCFG(_type='route', w=304, h=304, c=128, layers=(-1, -7))\n",
            "304 304 128\tconvCFG(_type='convolutional', w=304, h=304, c=128, size=1, stride=1, pad=0, filters=64)\n",
            "304 304 64\tconvCFG(_type='convolutional', w=304, h=304, c=64, size=3, stride=2, pad=1, filters=128)\n",
            "152 152 128\tconvCFG(_type='convolutional', w=152, h=152, c=128, size=1, stride=1, pad=0, filters=64)\n",
            "152 152 128\trouteCFG(_type='route', w=152, h=152, c=128, layers=(-2,))\n",
            "152 152 128\tconvCFG(_type='convolutional', w=152, h=152, c=128, size=1, stride=1, pad=0, filters=64)\n",
            "152 152 64\tconvCFG(_type='convolutional', w=152, h=152, c=64, size=1, stride=1, pad=0, filters=64)\n",
            "152 152 64\tconvCFG(_type='convolutional', w=152, h=152, c=64, size=3, stride=1, pad=1, filters=64)\n",
            "152 152 64\tshortcutCFG(_type='shortcut', w=152, h=152, c=64, _from=(-3,), activation='linear')\n",
            "152 152 64\tconvCFG(_type='convolutional', w=152, h=152, c=64, size=1, stride=1, pad=0, filters=64)\n",
            "152 152 64\tconvCFG(_type='convolutional', w=152, h=152, c=64, size=3, stride=1, pad=1, filters=64)\n",
            "152 152 64\tshortcutCFG(_type='shortcut', w=152, h=152, c=64, _from=(-3,), activation='linear')\n",
            "152 152 64\tconvCFG(_type='convolutional', w=152, h=152, c=64, size=1, stride=1, pad=0, filters=64)\n",
            "152 152 128\trouteCFG(_type='route', w=152, h=152, c=128, layers=(-1, -10))\n",
            "152 152 128\tconvCFG(_type='convolutional', w=152, h=152, c=128, size=1, stride=1, pad=0, filters=128)\n",
            "152 152 128\tconvCFG(_type='convolutional', w=152, h=152, c=128, size=3, stride=2, pad=1, filters=256)\n",
            "76 76 256\tconvCFG(_type='convolutional', w=76, h=76, c=256, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 256\trouteCFG(_type='route', w=76, h=76, c=256, layers=(-2,))\n",
            "76 76 256\tconvCFG(_type='convolutional', w=76, h=76, c=256, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=3, stride=1, pad=1, filters=128)\n",
            "76 76 128\tshortcutCFG(_type='shortcut', w=76, h=76, c=128, _from=(-3,), activation='linear')\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=3, stride=1, pad=1, filters=128)\n",
            "76 76 128\tshortcutCFG(_type='shortcut', w=76, h=76, c=128, _from=(-3,), activation='linear')\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=3, stride=1, pad=1, filters=128)\n",
            "76 76 128\tshortcutCFG(_type='shortcut', w=76, h=76, c=128, _from=(-3,), activation='linear')\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=3, stride=1, pad=1, filters=128)\n",
            "76 76 128\tshortcutCFG(_type='shortcut', w=76, h=76, c=128, _from=(-3,), activation='linear')\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=3, stride=1, pad=1, filters=128)\n",
            "76 76 128\tshortcutCFG(_type='shortcut', w=76, h=76, c=128, _from=(-3,), activation='linear')\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=3, stride=1, pad=1, filters=128)\n",
            "76 76 128\tshortcutCFG(_type='shortcut', w=76, h=76, c=128, _from=(-3,), activation='linear')\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=3, stride=1, pad=1, filters=128)\n",
            "76 76 128\tshortcutCFG(_type='shortcut', w=76, h=76, c=128, _from=(-3,), activation='linear')\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=3, stride=1, pad=1, filters=128)\n",
            "76 76 128\tshortcutCFG(_type='shortcut', w=76, h=76, c=128, _from=(-3,), activation='linear')\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 256\trouteCFG(_type='route', w=76, h=76, c=256, layers=(-1, -28))\n",
            "76 76 256\tconvCFG(_type='convolutional', w=76, h=76, c=256, size=1, stride=1, pad=0, filters=256)\n",
            "76 76 256\tconvCFG(_type='convolutional', w=76, h=76, c=256, size=3, stride=2, pad=1, filters=512)\n",
            "38 38 512\tconvCFG(_type='convolutional', w=38, h=38, c=512, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 512\trouteCFG(_type='route', w=38, h=38, c=512, layers=(-2,))\n",
            "38 38 512\tconvCFG(_type='convolutional', w=38, h=38, c=512, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=256)\n",
            "38 38 256\tshortcutCFG(_type='shortcut', w=38, h=38, c=256, _from=(-3,), activation='linear')\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=256)\n",
            "38 38 256\tshortcutCFG(_type='shortcut', w=38, h=38, c=256, _from=(-3,), activation='linear')\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=256)\n",
            "38 38 256\tshortcutCFG(_type='shortcut', w=38, h=38, c=256, _from=(-3,), activation='linear')\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=256)\n",
            "38 38 256\tshortcutCFG(_type='shortcut', w=38, h=38, c=256, _from=(-3,), activation='linear')\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=256)\n",
            "38 38 256\tshortcutCFG(_type='shortcut', w=38, h=38, c=256, _from=(-3,), activation='linear')\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=256)\n",
            "38 38 256\tshortcutCFG(_type='shortcut', w=38, h=38, c=256, _from=(-3,), activation='linear')\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=256)\n",
            "38 38 256\tshortcutCFG(_type='shortcut', w=38, h=38, c=256, _from=(-3,), activation='linear')\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=256)\n",
            "38 38 256\tshortcutCFG(_type='shortcut', w=38, h=38, c=256, _from=(-3,), activation='linear')\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 512\trouteCFG(_type='route', w=38, h=38, c=512, layers=(-1, -28))\n",
            "38 38 512\tconvCFG(_type='convolutional', w=38, h=38, c=512, size=1, stride=1, pad=0, filters=512)\n",
            "38 38 512\tconvCFG(_type='convolutional', w=38, h=38, c=512, size=3, stride=2, pad=1, filters=1024)\n",
            "19 19 1024\tconvCFG(_type='convolutional', w=19, h=19, c=1024, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 1024\trouteCFG(_type='route', w=19, h=19, c=1024, layers=(-2,))\n",
            "19 19 1024\tconvCFG(_type='convolutional', w=19, h=19, c=1024, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=3, stride=1, pad=1, filters=512)\n",
            "19 19 512\tshortcutCFG(_type='shortcut', w=19, h=19, c=512, _from=(-3,), activation='linear')\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=3, stride=1, pad=1, filters=512)\n",
            "19 19 512\tshortcutCFG(_type='shortcut', w=19, h=19, c=512, _from=(-3,), activation='linear')\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=3, stride=1, pad=1, filters=512)\n",
            "19 19 512\tshortcutCFG(_type='shortcut', w=19, h=19, c=512, _from=(-3,), activation='linear')\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=3, stride=1, pad=1, filters=512)\n",
            "19 19 512\tshortcutCFG(_type='shortcut', w=19, h=19, c=512, _from=(-3,), activation='linear')\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 1024\trouteCFG(_type='route', w=19, h=19, c=1024, layers=(-1, -16))\n",
            "19 19 1024\tconvCFG(_type='convolutional', w=19, h=19, c=1024, size=1, stride=1, pad=0, filters=1024)\n",
            "19 19 1024\tconvCFG(_type='convolutional', w=19, h=19, c=1024, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=3, stride=1, pad=1, filters=1024)\n",
            "19 19 1024\tconvCFG(_type='convolutional', w=19, h=19, c=1024, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 512\tmaxpoolCFG(_type='maxpool', w=19, h=19, c=512, stride=1, size=5)\n",
            "19 19 512\trouteCFG(_type='route', w=19, h=19, c=512, layers=(-2,))\n",
            "19 19 512\tmaxpoolCFG(_type='maxpool', w=19, h=19, c=512, stride=1, size=9)\n",
            "19 19 512\trouteCFG(_type='route', w=19, h=19, c=512, layers=(-4,))\n",
            "19 19 512\tmaxpoolCFG(_type='maxpool', w=19, h=19, c=512, stride=1, size=13)\n",
            "19 19 2048\trouteCFG(_type='route', w=19, h=19, c=2048, layers=(-1, -3, -5, -6))\n",
            "19 19 2048\tconvCFG(_type='convolutional', w=19, h=19, c=2048, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=3, stride=1, pad=1, filters=1024)\n",
            "19 19 1024\tconvCFG(_type='convolutional', w=19, h=19, c=1024, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=1, stride=1, pad=0, filters=256)\n",
            "19 19 256\tupsampleCFG(_type='upsample', w=19, h=19, c=256, stride=2)\n",
            "38 38 512\trouteCFG(_type='route', w=38, h=38, c=512, layers=(85,))\n",
            "38 38 512\tconvCFG(_type='convolutional', w=38, h=38, c=512, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 512\trouteCFG(_type='route', w=38, h=38, c=512, layers=(-1, -3))\n",
            "38 38 512\tconvCFG(_type='convolutional', w=38, h=38, c=512, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=512)\n",
            "38 38 512\tconvCFG(_type='convolutional', w=38, h=38, c=512, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=512)\n",
            "38 38 512\tconvCFG(_type='convolutional', w=38, h=38, c=512, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=1, stride=1, pad=0, filters=128)\n",
            "38 38 128\tupsampleCFG(_type='upsample', w=38, h=38, c=128, stride=2)\n",
            "76 76 256\trouteCFG(_type='route', w=76, h=76, c=256, layers=(54,))\n",
            "76 76 256\tconvCFG(_type='convolutional', w=76, h=76, c=256, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 256\trouteCFG(_type='route', w=76, h=76, c=256, layers=(-1, -3))\n",
            "76 76 256\tconvCFG(_type='convolutional', w=76, h=76, c=256, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=3, stride=1, pad=1, filters=256)\n",
            "76 76 256\tconvCFG(_type='convolutional', w=76, h=76, c=256, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=3, stride=1, pad=1, filters=256)\n",
            "76 76 256\tconvCFG(_type='convolutional', w=76, h=76, c=256, size=1, stride=1, pad=0, filters=128)\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=3, stride=1, pad=1, filters=256)\n",
            "76 76 256\tconvCFG(_type='convolutional', w=76, h=76, c=256, size=1, stride=1, pad=0, filters=255)\n",
            "76 76 255\tyoloCFG(_type='yolo', w=76, h=76, c=255, mask=(0, 1, 2), anchors=[(12, 16), (19, 36), (40, 28), (36, 75), (76, 55), (72, 146), (142, 110), (192, 243), (459, 401)], scale_x_y=1)\n",
            "76 76 128\trouteCFG(_type='route', w=76, h=76, c=128, layers=(-4,))\n",
            "76 76 128\tconvCFG(_type='convolutional', w=76, h=76, c=128, size=3, stride=2, pad=1, filters=256)\n",
            "38 38 512\trouteCFG(_type='route', w=38, h=38, c=512, layers=(-1, -16))\n",
            "38 38 512\tconvCFG(_type='convolutional', w=38, h=38, c=512, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=512)\n",
            "38 38 512\tconvCFG(_type='convolutional', w=38, h=38, c=512, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=512)\n",
            "38 38 512\tconvCFG(_type='convolutional', w=38, h=38, c=512, size=1, stride=1, pad=0, filters=256)\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=1, pad=1, filters=512)\n",
            "38 38 512\tconvCFG(_type='convolutional', w=38, h=38, c=512, size=1, stride=1, pad=0, filters=255)\n",
            "38 38 255\tyoloCFG(_type='yolo', w=38, h=38, c=255, mask=(3, 4, 5), anchors=[(12, 16), (19, 36), (40, 28), (36, 75), (76, 55), (72, 146), (142, 110), (192, 243), (459, 401)], scale_x_y=1)\n",
            "38 38 256\trouteCFG(_type='route', w=38, h=38, c=256, layers=(-4,))\n",
            "38 38 256\tconvCFG(_type='convolutional', w=38, h=38, c=256, size=3, stride=2, pad=1, filters=512)\n",
            "19 19 1024\trouteCFG(_type='route', w=19, h=19, c=1024, layers=(-1, -37))\n",
            "19 19 1024\tconvCFG(_type='convolutional', w=19, h=19, c=1024, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=3, stride=1, pad=1, filters=1024)\n",
            "19 19 1024\tconvCFG(_type='convolutional', w=19, h=19, c=1024, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=3, stride=1, pad=1, filters=1024)\n",
            "19 19 1024\tconvCFG(_type='convolutional', w=19, h=19, c=1024, size=1, stride=1, pad=0, filters=512)\n",
            "19 19 512\tconvCFG(_type='convolutional', w=19, h=19, c=512, size=3, stride=1, pad=1, filters=1024)\n",
            "19 19 1024\tconvCFG(_type='convolutional', w=19, h=19, c=1024, size=1, stride=1, pad=0, filters=255)\n",
            "19 19 255\tyoloCFG(_type='yolo', w=19, h=19, c=255, mask=(6, 7, 8), anchors=[(12, 16), (19, 36), (40, 28), (36, 75), (76, 55), (72, 146), (142, 110), (192, 243), (459, 401)], scale_x_y=1)\n",
            "bytes_read: 257717640, original_size: 257717640, final_position: 257717640\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.eager.def_function.Function at 0x7f7e7bb301d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWL8QjTb5HoN"
      },
      "source": [
        "Below is a function to infer the bounding boxes from an online image using the YOLO model. The frontend will pass the image from the webcam to the backend function in Colab by using a URI. When the backend function receives the image, it will decode the image into a Numpy array of integers (0 to 255) with 3 dimensions (width, height, RGB channels). Next, it will use the TensorFlow to normalize the pixels and resize the image. After that, it will use the model to predict the bounding boxes for any objects that may appear in the image. Finally, the bounding box format is converted to a JSON object so it can be returned and shown on the frontend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfXCscqgxqHP"
      },
      "source": [
        "# A OpenCV flag to treat the input image as an RGB image. This flag has a\n",
        "# different name in OpenCV 2 and OpenCV 3, so a condition is needed to work with\n",
        "# both versions.\n",
        "COLOR_IMAGE = cv2.IMREAD_COLOR if int(cv2.__version__.split('.', 1)[0]) >= 3 \\\n",
        "  else cv2.CV_LOAD_IMAGE_COLOR\n",
        "\n",
        "if model.backbone.model_id == 'cspdarknet53':\n",
        "  MODEL_INPUT_RESOLUTION = (608, 608)\n",
        "else:\n",
        "  MODEL_INPUT_RESOLUTION = (416, 416)\n",
        "DEMO_SCREEN_RESOLUTION = (500, 375)\n",
        "\n",
        "def yolo_infer(uri: str) -> JSON:\n",
        "  try:\n",
        "    # Decode the URI to an image\n",
        "    with urllib.request.urlopen(uri) as response:\n",
        "      data = response.read()\n",
        "    img_buf = np.frombuffer(data, np.uint8)\n",
        "    img = cv2.imdecode(img_buf, COLOR_IMAGE)\n",
        "    \n",
        "    # Rescale the image for use with the model\n",
        "    mat = tf.cast(img, tf.float16)\n",
        "    mat /= 255\n",
        "    mat = tf.expand_dims(mat, axis = 0)\n",
        "    mat = tf.image.resize(mat, MODEL_INPUT_RESOLUTION)\n",
        "\n",
        "    # Run the inference\n",
        "    a = time.time()\n",
        "    pred = model.predict(mat)\n",
        "    a = time.time() - a\n",
        "\n",
        "    # Rescale bounding boxes\n",
        "    bboxes, classes = demo_utils.int_scale_boxes(pred['bbox'], pred['classes'],\n",
        "                                                 *DEMO_SCREEN_RESOLUTION)\n",
        "\n",
        "    # Convert the format of the bounding boxes to match the format in the\n",
        "    # HTML document shown below: [x1, x2, y1, y2, c, p]\n",
        "    bboxes = bboxes[0].numpy()\n",
        "    classes = classes[0].numpy()\n",
        "    confidences = pred['confidence'][0]\n",
        "    num_dets = pred['num_dets'][0]\n",
        "    boxes = []\n",
        "    for i, bbox, class_id, confidence in zip(range(num_dets), bboxes, classes,\n",
        "                                             confidences):\n",
        "      boxes.append(list(bbox) + [class_id, confidence])\n",
        "    print('\\r', a, boxes, end='')\n",
        "\n",
        "    # Return control to the client side (JavaScript in the HTML document)\n",
        "    return JSON(boxes)\n",
        "  except Exception as e:\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "output.register_callback('yolo_infer', yolo_infer)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izo0wxEMMX42"
      },
      "source": [
        "Below is a frontend interface to access the webcam, and stream the images to the backend, and display the bounding boxes to the user. The stream uses JPEG compression to speed up the transfer of images to Colab. The `yolo_infer` function that was made earlier is then called on the compressed JPEG image and the resulting bounding boxes are then drawn on the webcam images when they are recieved back from the backend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWcPgacBSlvm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "outputId": "9652d687-6647-484e-d5b1-db23deca98f3"
      },
      "source": [
        "HTML(filename='utils/demos/colab_templates/inference.html')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<!--\n",
              "  Sources:\n",
              "    https://www.kirupa.com/html5/accessing_your_webcam_in_html5.htm\n",
              "    https://developer.mozilla.org/en-US/docs/Web/Guide/Audio_and_video_manipulation -->\n",
              "\n",
              "<!DOCTYPE html>\n",
              "<html>\n",
              "<head>\n",
              "<meta charset=\"utf-8\">\n",
              "<style>\n",
              "#container {\n",
              "\tmargin: 0px auto;\n",
              "\twidth: 500px;\n",
              "\theight: 375px;\n",
              "\tborder: 10px #333 solid;\n",
              "}\n",
              "#videoElement {\n",
              "\twidth: 500px;\n",
              "\theight: 375px;\n",
              "\tbackground-color: #666;\n",
              "}\n",
              "#my-canvas {\n",
              "\tbackground-color: #666;\n",
              "}\n",
              "</style>\n",
              "</head>\n",
              " \n",
              "<body>\n",
              "<div id=\"container\">\n",
              "  <canvas id=\"my-canvas\" width=\"500\" height=\"375\"></canvas>\n",
              "\t<video autoplay=\"true\" id=\"videoElement\" style=\"visibility:hidden\"></video>\n",
              "</div>\n",
              "\n",
              "<button id=\"toggleWebcam\">Start Webcam</button>\n",
              "\n",
              "<script src=\"https://code.jquery.com/jquery-3.5.1.min.js\" integrity=\"sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=\" crossorigin=\"anonymous\"></script>\n",
              "<script>\n",
              "  // Load COCO class names from the repo and give the classes seemingly random, but distinct, colors\n",
              "  var classes;\n",
              "  var colors = [];\n",
              "  $.ajax({\n",
              "    url: 'https://raw.githubusercontent.com/PurdueCAM2Project/TensorFlowModelGardeners/master/yolo/dataloaders/dataset_specs/coco.names',\n",
              "    success: function(data) {\n",
              "      classes = data.split('\\n');\n",
              "      for (var i = 0; i < classes.length; i++) {\n",
              "        colors.push(\"#\" + Math.round(0x1000000 * (i / classes.length)).toString(16));\n",
              "      }\n",
              "      for (var i = colors.length - 1; i >= 0; i--) {\n",
              "          j = Math.floor(Math.random() * (i + 1));\n",
              "          x = colors[i];\n",
              "          colors[i] = colors[j];\n",
              "          colors[j] = x;\n",
              "      }\n",
              "    }\n",
              "  });\n",
              "\n",
              "  var video = document.querySelector(\"#videoElement\");\n",
              "  var toggleWebcamButton = document.querySelector(\"#toggleWebcam\");\n",
              "  var camOn = false;\n",
              "\n",
              "  function startWebcam(e) {\n",
              "    camOn = true;\n",
              "    if (navigator.mediaDevices.getUserMedia) {\n",
              "      navigator.mediaDevices.getUserMedia({ video: true })\n",
              "        .then(function (stream) {\n",
              "          video.srcObject = stream;\n",
              "        })\n",
              "        .catch(function (err0r) {\n",
              "          console.log(\"Something went wrong!\");\n",
              "        });\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function stopWebcam(e) {\n",
              "    var stream = video.srcObject;\n",
              "    var tracks = stream.getTracks();\n",
              "\n",
              "    for (var i = 0; i < tracks.length; i++) {\n",
              "      var track = tracks[i];\n",
              "      track.stop();\n",
              "    }\n",
              "\n",
              "    video.srcObject = null;\n",
              "    camOn = false;\n",
              "  }\n",
              "\n",
              "  function toggleWebcam(e) {\n",
              "    if (camOn) {\n",
              "      stopWebcam(e);\n",
              "      toggleWebcamButton.innerText = \"Start Webcam\";\n",
              "    } else {\n",
              "      startWebcam(e);\n",
              "      toggleWebcamButton.innerText = \"Stop Webcam\";\n",
              "      processor.doLoad();\n",
              "    }\n",
              "  }\n",
              "\n",
              "  toggleWebcamButton.addEventListener(\"click\", toggleWebcam);\n",
              "\n",
              "  var processor = {  \n",
              "    timerCallback: async function() {  \n",
              "      if (!camOn) {  \n",
              "        return;  \n",
              "      }  \n",
              "      await this.computeFrame();  \n",
              "      var self = this;  \n",
              "      setTimeout(function () {  \n",
              "        self.timerCallback();  \n",
              "      }, 0);\n",
              "    },\n",
              "\n",
              "    doLoad: function() {\n",
              "      this.video = document.getElementById(\"videoElement\");\n",
              "      this.c1 = document.getElementById(\"my-canvas\");\n",
              "      this.ctx1 = this.c1.getContext(\"2d\");\n",
              "      this.lastFrameBoundingBoxes = [];\n",
              "\n",
              "      this.width = 500;  \n",
              "      this.height = 375;  \n",
              "      this.timerCallback();\n",
              "    },  \n",
              "\n",
              "    computeFrame: async function() {\n",
              "      this.ctx1.drawImage(this.video, 0, 0, this.width, this.height);\n",
              "      var frame = this.ctx1.getImageData(0, 0, this.width, this.height);\n",
              "\n",
              "      this.ctx1.putImageData(frame, 0, 0);\n",
              "      var url = this.c1.toDataURL('image/jpeg', 0.8);\n",
              "      this.drawBoundingBoxes(this.lastFrameBoundingBoxes);\n",
              "      var result = await google.colab.kernel.invokeFunction('yolo_infer', [url], {});\n",
              "      this.lastFrameBoundingBoxes = result.data['application/json'];\n",
              "      return;\n",
              "    },\n",
              "\n",
              "    drawBoundingBoxes: function(boxes) {\n",
              "      for (var i = 0; i < boxes.length; i++) {\n",
              "        const [x1, x2, y1, y2, c, p] = boxes[i];\n",
              "        console.log([classes[c] + \", \" + p, x1, y1]);\n",
              "        debugger;\n",
              "        this.ctx1.beginPath();\n",
              "        this.ctx1.lineWidth = \"2\";\n",
              "        this.ctx1.strokeStyle = colors[c];\n",
              "        this.ctx1.rect(x1, y1, x2-x1, y2-y1);\n",
              "        this.ctx1.stroke();\n",
              "        this.ctx1.font = \"18px Monospace\";\n",
              "        this.ctx1.fillStyle = colors[c];\n",
              "        this.ctx1.fillText(classes[c] + \", \" + p.toFixed(2), x1, y1 - 3);\n",
              "      }\n",
              "    }\n",
              "  };\n",
              "</script>\n",
              "</body>\n",
              "</html>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "stream",
          "text": [
            " 0.08053755760192871 [[20, 487, 70, 382, 0, 0.7905]]"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}